{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 46. Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizer class\n",
    "* 매개변수 갱신을 위한 기반클래스\n",
    "* 구체적인 최적화 기법은 optimizer 클래스를 상속한 곳에서 구현한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self):\n",
    "        self.target = None           # 업데이트 대상 : Model 또는 Layer\n",
    "        self.hooks = []\n",
    "        \n",
    "    def setup(self, target):         # setup 메서드에서 업데이트 타겟 설정\n",
    "        self.target = target\n",
    "        return self\n",
    "    \n",
    "    def update(self):\n",
    "        # gradient가 None이 아닌 모든 파라미터를 모은다.\n",
    "        params = [p for p in self.target.params() if p.grad is not None]\n",
    "        \n",
    "        # 전처리(option) : weight decay / gradient clipping 등\n",
    "        for f in self.hooks:\n",
    "            f(params)\n",
    "        \n",
    "        # 매개변수 갱신\n",
    "        for param in params:\n",
    "            self.update_one(param)\n",
    "    \n",
    "    def update_one(self, params):     # 구체적인 업데이트 방식은 자식클래스에서 재정의\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def add_hook(self, f):\n",
    "        self.hooks.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, lr = 0.01):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update_one(self, param):\n",
    "        param.data -= self.lr * param.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MomentumSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MomentumSGD(Optimizer):\n",
    "    def __init__(self, lr = 0.01, momentum = 0.9):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.vs = {}\n",
    "        \n",
    "    def update_one(self, param):\n",
    "        v_key = id(param)\n",
    "        if v_key not in self.vs:\n",
    "            self.vs[v_key] = np.zeros_like(param.data)\n",
    "            \n",
    "        v = self.vs[v_key]\n",
    "        v *= self.momentum\n",
    "        v -= self.lr * param.grad.data\n",
    "        param.data += v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(0.8165178492839196)\n",
      "variable(0.07743134827996008)\n",
      "variable(0.07544895146731474)\n",
      "variable(0.0746326030585864)\n",
      "variable(0.07420983776361521)\n",
      "variable(0.07397000396385317)\n",
      "variable(0.07383179319278566)\n",
      "variable(0.07375198316276851)\n",
      "variable(0.07370578149495666)\n",
      "variable(0.07367887538341852)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dezero import Variable\n",
    "from dezero import optimizers\n",
    "import dezero.functions as F\n",
    "from dezero.models import MLP\n",
    "\n",
    "# data\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)\n",
    "\n",
    "# parameter setting\n",
    "lr = 0.2\n",
    "iters = 10000\n",
    "hidden_size = 10\n",
    "\n",
    "# model define\n",
    "model = MLP((hidden_size, 1))\n",
    "optimizer = optimizers.MomentumSGD(lr).setup(model)\n",
    "\n",
    "# learning\n",
    "for i in range(iters):\n",
    "    y_pred = model(x)\n",
    "    loss = F.mean_squared_error(y, y_pred)\n",
    "\n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.update()\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
