{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 46. Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizer class\n",
    "* 매개변수 갱신을 위한 기반클래스\n",
    "* 구체적인 최적화 기법은 optimizer 클래스를 상속한 곳에서 구현한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self):\n",
    "        self.target = None           # 업데이트 대상 : Model 또는 Layer\n",
    "        self.hooks = []\n",
    "        \n",
    "    def setup(self, target):         # setup 메서드에서 업데이트 타겟 설정\n",
    "        self.target = target\n",
    "        return self\n",
    "    \n",
    "    def update(self):\n",
    "        # gradient가 None이 아닌 모든 파라미터를 모은다.\n",
    "        params = [p for p in self.target.params() if p.grad is not None]\n",
    "        \n",
    "        # 전처리(option) : weight decay / gradient clipping 등\n",
    "        for f in self.hooks:\n",
    "            f(params)\n",
    "        \n",
    "        # 매개변수 갱신\n",
    "        for param in params:\n",
    "            self.update_one(param)\n",
    "    \n",
    "    def update_one(self, params):     # 구체적인 업데이트 방식은 자식클래스에서 재정의\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def add_hook(self, f):\n",
    "        self.hooks.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, lr = 0.01):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update_one(self, param):\n",
    "        param.data -= self.lr * param.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MomentumSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MomentumSGD(Optimizer):\n",
    "    def __init__(self, lr = 0.01, momentum = 0.9):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.vs = {}\n",
    "        \n",
    "    def update_one(self, param):\n",
    "        v_key = id(param)\n",
    "        if v_key not in self.vs:\n",
    "            self.vs[v_key] = np.zeros_like(param.data)\n",
    "            \n",
    "        v = self.vs[v_key]\n",
    "        v *= self.momentum\n",
    "        v -= self.lr * param.grad.data\n",
    "        param.data += v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(0.8165178492839196)\n",
      "variable(0.07743134827996008)\n",
      "variable(0.07544895146731474)\n",
      "variable(0.0746326030585864)\n",
      "variable(0.07420983776361521)\n",
      "variable(0.07397000396385317)\n",
      "variable(0.07383179319278566)\n",
      "variable(0.07375198316276851)\n",
      "variable(0.07370578149495666)\n",
      "variable(0.07367887538341852)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dezero import Variable\n",
    "from dezero import optimizers\n",
    "import dezero.functions as F\n",
    "from dezero.models import MLP\n",
    "\n",
    "# data\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)\n",
    "\n",
    "# parameter setting\n",
    "lr = 0.2\n",
    "iters = 10000\n",
    "hidden_size = 10\n",
    "\n",
    "# model define\n",
    "model = MLP((hidden_size, 1))\n",
    "optimizer = optimizers.MomentumSGD(lr).setup(model)\n",
    "\n",
    "# learning\n",
    "for i in range(iters):\n",
    "    y_pred = model(x)\n",
    "    loss = F.mean_squared_error(y, y_pred)\n",
    "\n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.update()\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 47. Softmax & Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add utility function - get_item()\n",
    "# 다차원 배열을 slicing하는 기능\n",
    "\n",
    "import numpy as np\n",
    "from dezero import Variable\n",
    "import dezero.functions as F\n",
    "\n",
    "x = Variable(np.array([[1,2,3], [4,5,6]]))\n",
    "y = F.get_item(x, 1)\n",
    "print(y)\n",
    "\n",
    "y.backward()\n",
    "print(x.grad())\n",
    "\n",
    "indices = np.array([0,0,1])\n",
    "y = F.get_item(x, indices)\n",
    "print(y)\n",
    "\n",
    "# Variable.__getitem__ = F.get_item\n",
    "\n",
    "y = x[1]\n",
    "print(y)\n",
    "\n",
    "y = x[:,2]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implement softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[-0.54224404  0.09685547  0.52286398]\n",
      "          [-0.45705156  0.09128976  0.65779778]\n",
      "          [-0.7642369   0.21514968  0.05824683]])\n"
     ]
    }
   ],
   "source": [
    "from dezero.models import MLP\n",
    "import numpy as np\n",
    "\n",
    "model = MLP((10,3))\n",
    "\n",
    "x = np.array([[0.2,-0.4],[0.3,0.5],[1.3,-3.2]])\n",
    "y = model(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero import Variable. as_variable\n",
    "import dezero.functions as F\n",
    "\n",
    "def softmax1d(x):\n",
    "    x = as_variable(x)\n",
    "    y = F.exp(x)\n",
    "    sum_y = F.sum(y)\n",
    "    return y / sum_y\n",
    "\n",
    "x = Variable(np.array([[0.2, -0.4]]))\n",
    "y = model(x)\n",
    "p = softmax1d(y)\n",
    "print(y)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax for batch computation\n",
    "def softmax_simple(x, axis = 1):\n",
    "    y = as_variable(x)\n",
    "    y = exp(x)\n",
    "    sum_y = sum(y, axis = axis, keepdims = True)\n",
    "    eps = 1e-7\n",
    "    return y / (sum_y + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implement cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_simple(x, t):\n",
    "    x, t = as_variable(x), as_variable(t)\n",
    "    N = x.shape[0]\n",
    "    \n",
    "    p = softmax(x)\n",
    "    p = clip(p, 1e-15, 1.0)\n",
    "    log_p = log(p)\n",
    "    tlog_p = log_p[np.arange(N), t.data]\n",
    "    y = -1 * sum(tlog_p) / N\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(1.0828371573902766)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import dezero.functions as F\n",
    "\n",
    "X = np.array([[0.2,-0.4], [0.3, 0.5], [1.3,-3.2]])\n",
    "t = np.array([2, 0, 1])\n",
    "y = model(x)\n",
    "loss = F.softmax_cross_entropy(y, t)\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
